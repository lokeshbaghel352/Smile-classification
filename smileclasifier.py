# -*- coding: utf-8 -*-
"""Copy of Smileclasifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SI0pbwgAoeHbx9SEUrn_OQd2VREr-f7y
"""

import pandas as pd
import IPython.display as display
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import os,random
import matplotlib.pyplot as plt
print(tf.__version__)

from google.colab import drive
drive.mount('/content/drive/')

df=pd.read_csv("/content/drive/My Drive/ML Models/Smile/train.csv")
print(df.shape)
df

m=df.shape
category=[]
paths=[]
for i in range(m[0]):
    if(df['type'][i]=='negative smile'):
        category.append(0)
    elif(df['type'][i]=='positive smile'):
        category.append(1)
    else:
        category.append(2)
    x="/content/drive/My Drive/Smile/happy_images/"+df['path'][i]+".jpg"
    paths.append(x)
print(category)
print(paths)

df['Categories']=category
df['new_address']=paths
print(df)

IMAGE_HEIGHT=224
IMAGE_WIDTH=224
BATCH_SIZE=64

def load_and_preprocess_image(path):
  '''
  Load each image and resize it to desired shape
  '''
  image = tf.io.read_file(path)
  image = tf.image.decode_jpeg(image, channels=3)
  image = tf.image.resize(image, [IMAGE_WIDTH, IMAGE_HEIGHT])
  image /= 255.0  # normalize to [0,1] range
  return image

def convert_to_tensor(df):
  '''
  Convert each data and labels to tensor
  '''
  path_ds = tf.data.Dataset.from_tensor_slices(df['new_address'])
  image_ds = path_ds.map(load_and_preprocess_image)
  onehot_label=tf.one_hot(tf.cast(df['Categories'], tf.int64),3)
  #onehot_label=tf.cast(df['category'], tf.int64)
  label_ds = tf.data.Dataset.from_tensor_slices(onehot_label)
 
  return image_ds,label_ds
  print(convert_to_tensor(df))

X,Y=convert_to_tensor(df)
print("Shape of X in data:", X)
print("Shape of Y in data:", Y)

dataset=tf.data.Dataset.zip((X,Y)).shuffle(buffer_size=1000)
dataset_train=dataset.take(3831)
dataset_test=dataset.skip(3831)

dataset_train=dataset_train.batch(BATCH_SIZE, drop_remainder=True)
dataset_test=dataset_test.batch(BATCH_SIZE, drop_remainder=True)
dataset_train

def plotimages(imagesls):
  fig, axes = plt.subplots(1, 5, figsize=(20,20))
  axes = axes.flatten()
  for image,ax in zip(imagesls, axes):
    ax.imshow(image)
    ax.axis('off')

imagesls=[]
for n, image in enumerate(X.take(5)):
  imagesls.append(image)

plotimages(imagesls)

from tensorflow.keras.applications import DenseNet169

pre_trained_model = DenseNet169(input_shape=(IMAGE_WIDTH,IMAGE_HEIGHT,3), include_top=False, weights="imagenet")

pre_trained_model.trainable = True

for layer in pre_trained_model.layers:
  if 'conv5' in layer.name:
    layer.trainable = True
  else:
    layer.trainable = False

pre_trained_model.summary()

last_layer = pre_trained_model.get_layer('relu')
last_output = last_layer.output
x = layers.Flatten()(last_output)
x = layers.BatchNormalization()(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.Dropout(0.4)(x)
x = layers.BatchNormalization()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.4)(x)
x = layers.Dense(3, activation='softmax')(x)
model = tf.keras.models.Model(pre_trained_model.input, x)
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(lr=1e-4),
              metrics=['accuracy'])

model.summary()

hist=model.fit_generator(dataset_train,epochs=20,validation_data=dataset_test)
# model.fit(X_train, Y_train, epochs=20, validation_data=(X_val, Y_val), batch_size=32, verbose=1)

def plot_model_history(model_history, acc='accuracy', val_acc='val_accuracy'):
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    axs[0].plot(range(1,len(model_history.history[acc])+1),model_history.history[acc])
    axs[0].plot(range(1,len(model_history.history[val_acc])+1),model_history.history[val_acc])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    #axs[0].set_xticks(np.arange(1,len(model_history.history[acc])+1),len(model_history.history[acc])/10)
    axs[0].legend(['train', 'val'], loc='best')
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    #axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['train', 'val'], loc='best')
    plt.show()
    
plot_model_history(hist)

df_t=pd.read_csv("/content/drive/My Drive/Smile/test.csv")

m_t=df_t.shape
category_t=[]
paths_t=[]
for i in range(m_t[0]):
    if(df_t['label'][i]=='negative smile'):
        category_t.append(0)
    elif(df_t['label'][i]=='positive smile'):
        category_t.append(1)
    else:
        category_t.append(2)
    x="/content/drive/My Drive/Smile/happy_images/"+df_t['address'][i]+".jpg"
    paths_t.append(x)

df_t['Categories']=category_t
df_t['new_address']=paths_t

X_t,Y_t=convert_to_tensor(df_t)
print("Shape of X in data:", X_t)
print("Shape of Y in data:", Y_t)

imagesls_t=[]
for n, image in enumerate(X_t.take(5)):
  imagesls_t.append(image)
plotimages(imagesls_t)

dataset_t=tf.data.Dataset.zip((X_t,Y_t)).shuffle(buffer_size=200)
dataset_tr=dataset_t.batch(1, drop_remainder=True)

result = model.evaluate(dataset_tr)

vgg_y_pred =  model.predict_generator(dataset_tr)
test_data=dataset_tr.unbatch()
y_g=[]
for image, label in  test_data:
  y_g.append(label.numpy())

import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
# compute the confusion matrix
confusion_mtx = confusion_matrix(np.asarray(y_g).argmax(axis=1), np.asarray(vgg_y_pred).argmax(axis=1))
# plot the confusion matrix
f,ax = plt.subplots(figsize=(8, 8))
sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap="Blues",linecolor="gray", fmt= '.1f',ax=ax)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

report = classification_report(np.asarray(y_g).argmax(axis=1), np.asarray(vgg_y_pred).argmax(axis=1), target_names=['0','1','2'])
print(report)

